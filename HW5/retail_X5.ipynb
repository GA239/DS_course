{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV, cross_val_score, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from functools import partial\n",
    "import itertools\n",
    "from sklift.models import ClassTransformation\n",
    "import lightgbm as lgbm\n",
    "import ipywidgets as widgets\n",
    "import inspect\n",
    "from datetime import timedelta\n",
    "from sklearn import preprocessing\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('lg')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# add ch to logger\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-30 18:15:59,189 - lg - INFO - info message\n"
     ]
    }
   ],
   "source": [
    "logger.info('info message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_TRANSACTION_TEMPLATE = 'base_transaction'\n",
    "FAVORITES_TEMPLATE = 'favorites'\n",
    "STEPS_MAPPING = {\n",
    "    'BASE': (False, 'base_features.csv'),\n",
    "    'BASE_TRANSACTION': (False, BASE_TRANSACTION_TEMPLATE),\n",
    "    'FAVORITES': (False, FAVORITES_TEMPLATE),\n",
    "    'POPULARITY': (False, 'popularity'),\n",
    "    'ALCO': (True, 'alco'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_file_name(prefix, offset):\n",
    "    return '{}_{}.csv'.format(prefix, str(offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_from_files(offsets):\n",
    "    features = pd.read_csv(STEPS_MAPPING['BASE'][1], index_col='client_id')\n",
    "    \n",
    "    base_trans_array = []\n",
    "    for offset in offsets:\n",
    "        offset = offset or ''\n",
    "        base_trans_array.append(pd.read_csv(generate_file_name(STEPS_MAPPING['BASE_TRANSACTION'][1], offset), \n",
    "                                 index_col='client_id'))\n",
    "    \n",
    "    for df in base_trans_array:\n",
    "        features = features.merge(df, left_index=True, right_index=True)\n",
    "        del df\n",
    "\n",
    "    gc.collect()\n",
    "    favorites_array = []\n",
    "    for offset in offsets:\n",
    "        offset = offset or ''\n",
    "        favorites_array.append(pd.read_csv(generate_file_name(STEPS_MAPPING['FAVORITES'][1], offset), \n",
    "                                           index_col='client_id'))\n",
    "    \n",
    "    for df in favorites_array:\n",
    "        features = features.merge(df, left_index=True, right_index=True)\n",
    "        del df\n",
    "    gc.collect()\n",
    "\n",
    "    popularity_array = []\n",
    "    for offset in offsets:\n",
    "        offset = offset or ''\n",
    "        popularity_array.append(pd.read_csv(generate_file_name(STEPS_MAPPING['POPULARITY'][1], offset), \n",
    "                                           index_col='client_id'))\n",
    "    \n",
    "    for df in popularity_array:\n",
    "        features = features.merge(df, left_index=True, right_index=True)\n",
    "        del df\n",
    "    gc.collect()\n",
    "\n",
    "    alc = pd.read_csv(generate_file_name(STEPS_MAPPING['ALCO'][1], None), index_col='client_id')\n",
    "    features = features.merge(alc, left_index=True, right_index=True)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uplift_score(prediction, treatment, target, rate=0.3):\n",
    "    \"\"\"\n",
    "    Подсчет Uplift Score\n",
    "    \"\"\"\n",
    "    order = np.argsort(-prediction)\n",
    "\n",
    "    treatment_n = int((treatment == 1).sum() * rate)\n",
    "    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n",
    "\n",
    "    control_n = int((treatment == 0).sum() * rate)\n",
    "    control_p = target[order][treatment[order] == 0][:control_n].mean()\n",
    "\n",
    "    score = treatment_p - control_p\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_train_test(features, df_train, df_test):\n",
    "    return features.loc[df_train.index, :], features.loc[df_test.index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_learn(X_learn, y_learn):\n",
    "    _, treatment_counts = np.unique(y_learn.treatment_flg, return_counts=True)\n",
    "    logger.info(\"{}, {}, {}\".format(X_learn.shape, y_learn.shape, treatment_counts[0] - treatment_counts[1]))\n",
    "    \n",
    "    treat_learn = y_learn.treatment_flg\n",
    "    vc = treat_learn.value_counts()\n",
    "    treat_learn = pd.concat([treat_learn[treat_learn == i].sample(vc.min()) for i in vc.index])\n",
    "\n",
    "    X_learn = X_learn.loc[treat_learn.index, :]\n",
    "    y_learn = y_learn.loc[treat_learn.index, :]\n",
    "    \n",
    "    _, treatment_counts = np.unique(y_learn.treatment_flg, return_counts=True)\n",
    "    logger.info(\"{}, {}, {}\".format(X_learn.shape, y_learn.shape, treatment_counts[0] - treatment_counts[1]))\n",
    "    return X_learn, y_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uplift_score_func(y_true, y_pred, **kwargs):\n",
    "    return uplift_score(y_pred, treatment=y_true.treatment_flg, target=y_true.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyClassTransformation(ClassTransformation):\n",
    "    def fit(self, X, y, estimator_fit_params=None):\n",
    "        return  super().fit(X, y=y.target, treatment=y.treatment_flg, estimator_fit_params=estimator_fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_transactions_and_products(products, transactions):\n",
    "    columns = ['brand_id', 'vendor_id', 'segment_id', 'product_id']\n",
    "    transactions_with_products = transactions.merge(products, left_on='product_id', right_index=True)\n",
    "    logger.info('transactions_with_products')\n",
    "    for col in columns:\n",
    "        fg = transactions_with_products.drop_duplicates(subset=['client_id', col]) \\\n",
    "                                       .groupby([col]).size().sort_values(ascending=False)\n",
    "        fg.name = 'popularity_{}'.format(col)\n",
    "        \n",
    "        if col == 'product_id':\n",
    "            products = products.merge(fg, left_index=True, right_index=True)\n",
    "        else:\n",
    "            products = products.join(fg, on=col)\n",
    "        logger.info('popularity {}'.format(col))\n",
    "    new_columns = ['popularity_{}'.format(col) for col in columns]\n",
    "    transactions_with_products = transactions_with_products.merge(products[new_columns], left_on='product_id', right_index=True)\n",
    "    return transactions_with_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transactions(df_purchases, offset=None):\n",
    "    if not offset:\n",
    "        return df_purchases\n",
    "\n",
    "    last_date = df_purchases.date.max()    \n",
    "    sub_df_purchases = df_purchases[df_purchases.date > last_date-timedelta(days=offset)]\n",
    "    logger.info(\"sub_df_purchases shape : {}\".format(sub_df_purchases.shape))\n",
    "    return sub_df_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чтение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_products = pd.read_csv('data/products.csv', index_col='product_id')\n",
    "# logger.info(\"df_products shape {}\".format(df_products.shape))\n",
    "\n",
    "# # df_purchases = pd.read_csv('data/purchases.csv', index_col='transaction_id', parse_dates=['transaction_datetime'], nrows=100000)\n",
    "# df_purchases = pd.read_csv('data/purchases.csv', index_col='transaction_id', parse_dates=['transaction_datetime'])\n",
    "# df_purchases['date'] = df_purchases['transaction_datetime'].dt.date\n",
    "# logger.info(\"df_purchases shape {}\".format(df_purchases.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_products_features(trans):\n",
    "    columns = ['brand_id', 'vendor_id', 'segment_id', 'product_id']\n",
    "    columns2 = ['popularity_{}'.format(col) for col in columns]\n",
    "    total_pop = trans.groupby('client_id')[columns2].sum()\n",
    "    logger.info(total_pop.columns[0])\n",
    "    yield total_pop\n",
    "    \n",
    "    avg_trans_pop = trans.groupby(['client_id', 'transaction_id'])[columns2].sum().groupby(['client_id'])[columns2].mean()\n",
    "    avg_trans_pop.columns = ['avg_trans_{}'.format(c) for c in avg_trans_pop.columns]\n",
    "    logger.info(avg_trans_pop.columns[0])\n",
    "    yield avg_trans_pop\n",
    "\n",
    "    total_unique = trans.groupby(['client_id'])[columns].nunique()\n",
    "    total_unique.columns = ['total_unique_{}'.format(c) for c in total_unique.columns]\n",
    "    logger.info(total_unique.columns[0])\n",
    "    yield total_unique\n",
    "\n",
    "    avg_trans_unique = trans.groupby(['client_id', 'transaction_id'])[columns].nunique().groupby(['client_id'])[columns].mean()\n",
    "    avg_trans_unique.columns = ['avg_trans_unique_{}'.format(c) for c in avg_trans_unique.columns]\n",
    "    logger.info(avg_trans_unique.columns[0])\n",
    "    yield avg_trans_unique\n",
    "    \n",
    "    result = []\n",
    "    for c in columns[:-1]:\n",
    "        fc = trans.groupby(['client_id', c])['product_id'].nunique().groupby(['client_id']).mean()\n",
    "        fc.name = 'avg_nunique_prod_in_{}'.format(c)\n",
    "        result.append(fc)\n",
    "    avg_nunique_prod = pd.concat(result, axis=1, sort=False)\n",
    "    logger.info(avg_nunique_prod.columns[0])\n",
    "    yield avg_nunique_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_features(offset):\n",
    "    offset = offset or ''\n",
    "    final = None\n",
    "    for df in get_products_features(df_purchases):    \n",
    "        final = final.merge(df, left_index=True, right_index=True) if final is not None else df\n",
    "    final.index.name = 'client_id'\n",
    "    if STEPS_MAPPING['POPULARITY'][0]:\n",
    "        final.to_csv(generate_file_name(STEPS_MAPPING['POPULARITY'][1], offset))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_features(clean=True):\n",
    "    df_features = pd.read_csv('data/clients.csv', index_col='client_id', parse_dates=['first_issue_date','first_redeem_date'])\n",
    "    df_features['gender'] = LabelEncoder().fit_transform(df_features.gender)\n",
    "    df_features['first_issue_time'] = pd.to_datetime(df_features['first_issue_date']).astype(int) / 10 ** 9\n",
    "    df_features['first_redeem_time'] = pd.to_datetime(df_features['first_redeem_date']).astype(int) / 10 ** 9\n",
    "    df_features['issue_redeem_delay'] = df_features['first_redeem_time'] - df_features['first_issue_time']\n",
    "    df_features = df_features.drop(['first_issue_date', 'first_redeem_date'], axis=1)\n",
    "    if STEPS_MAPPING['BASE'][0]:\n",
    "        df_features.to_csv(STEPS_MAPPING['BASE'][1])\n",
    "    \n",
    "    if clean:\n",
    "        del df_features\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transactions_features(transactions, offset=None, clean=True):\n",
    "    offset = offset or ''\n",
    "    last_cols = [\n",
    "        'regular_points_received', \n",
    "        'express_points_received',\n",
    "        'regular_points_spent',\n",
    "        'express_points_spent',\n",
    "        'purchase_sum'\n",
    "    ]\n",
    "\n",
    "    logger.info(\"Create history\")\n",
    "    history = transactions.groupby(['client_id', 'transaction_id'])[last_cols].last()\n",
    "    \n",
    "    logger.info(\"Create _features\")\n",
    "    _features = [\n",
    "        (history.groupby('client_id')['purchase_sum'].count(), ['total_trans_count']), \n",
    "        (history.groupby('client_id').sum(), last_cols)\n",
    "    ]\n",
    "    \n",
    "    _features = list(zip(*_features))\n",
    "    transactions_features =  pd.concat(_features[0], axis = 1)\n",
    "    transactions_features.columns = list(itertools.chain.from_iterable(_features[1]))\n",
    "    transactions_features.columns = ['days_{}_'.format(str(offset)) + c for c in transactions_features.columns]\n",
    "    if STEPS_MAPPING['BASE_TRANSACTION'][0]:\n",
    "        transactions_features.to_csv(generate_file_name(STEPS_MAPPING['BASE_TRANSACTION'][1], offset))\n",
    "    \n",
    "    if clean:\n",
    "        del transactions_features\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    \n",
    "    return transactions_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def favorite_products_features(merged_transactions, offset=None, clean=True):\n",
    "\n",
    "    offset = offset or ''\n",
    "    # вычисляем любимый продукт/категорию/сегмент для каждого юзера.\n",
    "    cols = ['product_id', 'brand_id', 'vendor_id', 'segment_id']\n",
    "    result = []\n",
    "    for c in cols:\n",
    "        logger.info(\"favorite {}\".format(c))\n",
    "        result.append(\n",
    "            merged_transactions.groupby(['client_id', c]).size().reset_index(name='counts').groupby(['client_id']).max()[c]\n",
    "        )\n",
    "\n",
    "    favorites = pd.concat(result, axis=1, sort=False)\n",
    "    favorites.columns = [str(offset) + '_faivorite_' + c for c in cols]\n",
    "    favorites.index.name = 'client_id'\n",
    "\n",
    "    for col in favorites.columns:\n",
    "        logger.info('LabelEncoder for {}'.format(col))\n",
    "        favorites[col] = LabelEncoder().fit_transform(favorites[col].astype(str))    \n",
    "\n",
    "    if STEPS_MAPPING['FAVORITES'][0]:\n",
    "        favorites.to_csv(generate_file_name(STEPS_MAPPING['FAVORITES'][1], offset))\n",
    "\n",
    "    if clean:\n",
    "        del favorites\n",
    "        gc.collect()\n",
    "        return        \n",
    "        \n",
    "    return favorites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_the_first_part_of_features(offset, df_products, df_purchases):\n",
    "    sb_df_purchases = get_transactions(df_purchases, offset=offset)\n",
    "    transactions_features(sb_df_purchases, offset=offset)\n",
    "    sb_df_purchases = merge_transactions_and_products(df_products, sb_df_purchases)\n",
    "    favorite_products_features(merged_transactions=sb_df_purchases, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_alcohol_and_trade_mark(df_products, df_purchases):\n",
    "    return df_purchases[[\n",
    "        \"client_id\", \n",
    "        \"product_id\"\n",
    "    ]].merge(\n",
    "        df_products[[\"is_own_trademark\",\"is_alcohol\"]], \n",
    "        left_on='product_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg_alvo_features(alk_dt, columns):\n",
    "    tmp = alk_dt.groupby(columns).agg({\n",
    "    'product_id' : 'count', \n",
    "    'is_alcohol' : 'sum', \n",
    "    'is_own_trademark' : 'sum'})\n",
    "    tmp['part_own_trademark'] = tmp.is_own_trademark / tmp.product_id\n",
    "    tmp['part_is_alcohol'] = tmp.is_alcohol / tmp.product_id\n",
    "    \n",
    "    return_columns = ['is_own_trademark', 'part_own_trademark', 'is_alcohol', 'part_is_alcohol']\n",
    "    if len(columns) > 1:\n",
    "        result = tmp.groupby(columns[:1:]).mean()[return_columns]\n",
    "        result.columns = ['trans_avg_{}'.format(c) for c in return_columns]\n",
    "    else:\n",
    "        result = tmp[return_columns]\n",
    "    \n",
    "    return  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_alco_features():\n",
    "    alco_data = prepare_data_for_alcohol_and_trade_mark(df_products, df_purchases)\n",
    "    avg_trans_features = agg_alvo_features(alco_data, ['client_id', 'transaction_id'])\n",
    "    history_features = agg_alvo_features(alco_data, ['client_id'])\n",
    "    alco_features = avg_trans_features.merge(history_features, left_index=True, right_index=True)\n",
    "    if STEPS_MAPPING['ALCO'][0]:\n",
    "        alco_features.to_csv(generate_file_name(STEPS_MAPPING['ALCO'][1], None))\n",
    "    return alco_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base_features(df_clients);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# offets = [None]\n",
    "# for offst in offets:\n",
    "#     logger.info('{} offst = {}'.format('--'*30, offst))\n",
    "#     generate_the_first_part_of_features(offst, df_products, df_purchases);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# offset = 14\n",
    "# df_purchases = merge_transactions_and_products(df_products, get_transactions(df_purchases, offset=offset))\n",
    "# tpf = pop_features(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = prepare_alco_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "offets = [14, 30, None]\n",
    "features = get_features_from_files(offets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>first_issue_time</th>\n",
       "      <th>first_redeem_time</th>\n",
       "      <th>issue_redeem_delay</th>\n",
       "      <th>days_14_total_trans_count</th>\n",
       "      <th>days_14_regular_points_received</th>\n",
       "      <th>days_14_express_points_received</th>\n",
       "      <th>days_14_regular_points_spent</th>\n",
       "      <th>days_14_express_points_spent</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_nunique_prod_in_vendor_id</th>\n",
       "      <th>avg_nunique_prod_in_segment_id</th>\n",
       "      <th>trans_avg_is_own_trademark</th>\n",
       "      <th>trans_avg_part_own_trademark</th>\n",
       "      <th>trans_avg_is_alcohol</th>\n",
       "      <th>trans_avg_part_is_alcohol</th>\n",
       "      <th>is_own_trademark</th>\n",
       "      <th>part_own_trademark</th>\n",
       "      <th>is_alcohol</th>\n",
       "      <th>part_is_alcohol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>client_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000012768d</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>1.501948e+09</td>\n",
       "      <td>1.515094e+09</td>\n",
       "      <td>1.314656e+07</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.586207</td>\n",
       "      <td>1.956522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000036f903</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1.491832e+09</td>\n",
       "      <td>1.492951e+09</td>\n",
       "      <td>1.118613e+06</td>\n",
       "      <td>3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>2.268293</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.091865</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>14</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000048b7a6</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1.544881e+09</td>\n",
       "      <td>-9.223372e+09</td>\n",
       "      <td>-1.076825e+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.095238</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000073194a</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1.495544e+09</td>\n",
       "      <td>1.511522e+09</td>\n",
       "      <td>1.597811e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619048</td>\n",
       "      <td>2.370370</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.253947</td>\n",
       "      <td>3</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>9</td>\n",
       "      <td>0.109756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00007f9014</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1.503409e+09</td>\n",
       "      <td>1.550258e+09</td>\n",
       "      <td>4.684946e+07</td>\n",
       "      <td>2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.651163</td>\n",
       "      <td>1.763158</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.035632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  gender  first_issue_time  first_redeem_time  \\\n",
       "client_id                                                      \n",
       "000012768d   45       2      1.501948e+09       1.515094e+09   \n",
       "000036f903   72       0      1.491832e+09       1.492951e+09   \n",
       "000048b7a6   68       0      1.544881e+09      -9.223372e+09   \n",
       "000073194a   60       0      1.495544e+09       1.511522e+09   \n",
       "00007f9014   45       0      1.503409e+09       1.550258e+09   \n",
       "\n",
       "            issue_redeem_delay  days_14_total_trans_count  \\\n",
       "client_id                                                   \n",
       "000012768d        1.314656e+07                          2   \n",
       "000036f903        1.118613e+06                          3   \n",
       "000048b7a6       -1.076825e+10                          1   \n",
       "000073194a        1.597811e+07                          1   \n",
       "00007f9014        4.684946e+07                          2   \n",
       "\n",
       "            days_14_regular_points_received  days_14_express_points_received  \\\n",
       "client_id                                                                      \n",
       "000012768d                             10.0                              0.0   \n",
       "000036f903                              4.1                              0.0   \n",
       "000048b7a6                              1.2                              0.0   \n",
       "000073194a                              1.3                              0.0   \n",
       "00007f9014                              3.3                              0.0   \n",
       "\n",
       "            days_14_regular_points_spent  days_14_express_points_spent  ...  \\\n",
       "client_id                                                               ...   \n",
       "000012768d                           0.0                           0.0  ...   \n",
       "000036f903                           0.0                           0.0  ...   \n",
       "000048b7a6                           0.0                           0.0  ...   \n",
       "000073194a                           0.0                           0.0  ...   \n",
       "00007f9014                           0.0                           0.0  ...   \n",
       "\n",
       "            avg_nunique_prod_in_vendor_id  avg_nunique_prod_in_segment_id  \\\n",
       "client_id                                                                   \n",
       "000012768d                       1.586207                        1.956522   \n",
       "000036f903                       2.181818                        2.268293   \n",
       "000048b7a6                       2.095238                        2.625000   \n",
       "000073194a                       1.619048                        2.370370   \n",
       "00007f9014                       1.651163                        1.763158   \n",
       "\n",
       "            trans_avg_is_own_trademark  trans_avg_part_own_trademark  \\\n",
       "client_id                                                              \n",
       "000012768d                    1.000000                      0.064668   \n",
       "000036f903                    0.437500                      0.091865   \n",
       "000048b7a6                    0.375000                      0.041667   \n",
       "000073194a                    0.176471                      0.030303   \n",
       "00007f9014                    0.137931                      0.035632   \n",
       "\n",
       "            trans_avg_is_alcohol  trans_avg_part_is_alcohol  is_own_trademark  \\\n",
       "client_id                                                                       \n",
       "000012768d              0.000000                   0.000000                 4   \n",
       "000036f903              0.031250                   0.004464                14   \n",
       "000048b7a6              0.000000                   0.000000                 3   \n",
       "000073194a              0.529412                   0.253947                 3   \n",
       "00007f9014              0.000000                   0.000000                 4   \n",
       "\n",
       "            part_own_trademark  is_alcohol  part_is_alcohol  \n",
       "client_id                                                    \n",
       "000012768d            0.076923           0         0.000000  \n",
       "000036f903            0.086420           1         0.006173  \n",
       "000048b7a6            0.053571           0         0.000000  \n",
       "000073194a            0.036585           9         0.109756  \n",
       "00007f9014            0.036036           0         0.000000  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346831, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9e1622b385b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/uplift_train.csv', index_col='client_id')\n",
    "logger.info(df_train.shape)\n",
    "\n",
    "df_test = pd.read_csv('data/uplift_test.csv', index_col='client_id')\n",
    "logger.info(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = get_train_test(features, df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices_learn, indices_valid = train_test_split(x_train.index, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_learn = x_train.loc[indices_learn, :]\n",
    "y_learn = df_train.loc[indices_learn, :]\n",
    "\n",
    "X_val = x_train.loc[indices_valid, :]\n",
    "y_val = df_train.loc[indices_valid, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'learning_rate':0.01,'max_depth':4,'num_leaves':20,\n",
    "             'min_data_in_leaf':3, 'application':'binary', 'subsample':0.8, 'colsample_bytree': 0.8,\n",
    "             'reg_alpha':0.01,'data_random_seed':42,'metric':'binary_logloss',\n",
    "             'max_bin':416,'bagging_freq':3,'reg_lambda':0.01             \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = lgbm.Dataset(X_learn, label=y_learn.target)\n",
    "cv_result = lgbm.cv(params, matrix, num_boost_round=2000,nfold=5, stratified=True, \n",
    "                    shuffle=True, early_stopping_rounds=50, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_result['binary_logloss-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params['n_estimators'] = len(cv_result['binary_logloss-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(10, 40, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "parameters = {\n",
    "    'classifier__estimator__max_depth': [3, 4, 5],\n",
    "    'classifier__estimator__num_leaves': list(range(10, 40, 10)),\n",
    "    'classifier__estimator__min_child_samples': list(range(10, 40, 10)),\n",
    "    'classifier__estimator__class_weight': ['balanced', None]\n",
    "\n",
    "}\n",
    "modelcv = GridSearchCV(\n",
    "    Pipeline(steps=[\n",
    "        ('classifier', MyClassTransformation(lgbm.LGBMClassifier(**params)))\n",
    "    ]),\n",
    "    parameters,\n",
    "    scoring=make_scorer(uplift_score_func), \n",
    "    cv=ShuffleSplit(n_splits=4, test_size=0.3, random_state=12), \n",
    "    verbose=3, n_jobs=-1\n",
    ")\n",
    "modelcv.fit(X_learn, y_learn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = modelcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation score:', uplift_score(final_model.predict(X_val), treatment=y_val.treatment_flg, target=y_val.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    final_model, x_train, df_train,\n",
    "    cv=ShuffleSplit(n_splits=10, test_size=0.3), \n",
    "    scoring=make_scorer(uplift_score_func)\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### вычислим доверительный интервал оценки прогноза, чтобы по Public отсеживать overfit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores), st.sem(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.t.interval(0.95, len(scores)-1, loc=np.mean(scores), scale=st.sem(scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка предсказаний для тестовых клиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, df_train = balance_learn(x_train, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(x_train, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upl_sc = final_model.predict(x_test)\n",
    "pd.DataFrame({'client_id':x_test.index.values,'uplift': upl_sc}).to_csv('final_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({\n",
    "    'feature_score': final_model.steps[0][1].estimator.feature_importances_\n",
    "}, index=x_train.columns).sort_values('feature_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = fi.tail(15).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(20, 100, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local PySpark (Python-3.5 / Spark-2.3.0 )",
   "language": "python",
   "name": "py3spark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
