{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import contextmanager\n",
    "from functools import wraps\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.5.2 (default, Oct  8 2019, 13:06:37) \n",
      "[GCC 5.4.0 20160609]\n",
      "Version info.\n",
      "sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Python version\")\n",
    "print (sys.version)\n",
    "print(\"Version info.\")\n",
    "print (sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save(mapping_dict, name):\n",
    "    with open('data/{}.pickle'.format(name), 'wb') as handle:\n",
    "        pickle.dump(mapping_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load(name):\n",
    "    with open('data/{}.pickle'.format(name), 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "    \n",
    "\n",
    "def save_result(file_name=None, calculate=False, skip=False):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if calculate:\n",
    "                result = func(*args, **kwargs)\n",
    "                if file_name:\n",
    "                    if isinstance(result, pd.DataFrame):\n",
    "                        result.to_parquet('data/{}.parquet'.format(file_name))\n",
    "                    else:\n",
    "                        save(result, file_name)\n",
    "                return result\n",
    "            else:\n",
    "                if not skip and file_name:\n",
    "                    if os.path.isfile('data/{}.parquet'.format(file_name)) :\n",
    "                        return pd.read_parquet('data/{}.parquet'.format(file_name))\n",
    "                    else:\n",
    "                        return load(file_name)                    \n",
    "                else:\n",
    "                    display('code skipped')\n",
    "                    return\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_clusters(obj, mapping):\n",
    "    for v in mapping.values():\n",
    "        for i in obj:\n",
    "            if v.startswith(i):\n",
    "                yield i\n",
    "\n",
    "@save_result('mydata')\n",
    "def load_dataset():\n",
    "    mydata = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), shuffle=True)\n",
    "    display('Training data size:', len(mydata['data']))\n",
    "    return mydata \n",
    "\n",
    "@save_result('mappings')\n",
    "def generate_mappings(mydata):\n",
    "    a = [np.unique(['.'.join(i.split('.')[:k]) for i in mydata.target_names]) for k in range(1,4)]\n",
    "    targets = np.unique(mydata.target)\n",
    "    mapping1 = dict(zip(targets, a[2]))\n",
    "    mapping2 = dict(zip(targets, generate_new_clusters(a[1], mapping1)))\n",
    "    mapping3 = dict(zip(targets, generate_new_clusters(a[0], mapping1)))\n",
    "    return mapping1, mapping2, mapping3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'comp.os.ms-windows', 3: 'comp.sys.ibm', 4: 'comp.sys.mac', 5: 'comp.windows.x', 6: 'misc.forsale', 7: 'rec.autos', 8: 'rec.motorcycles', 9: 'rec.sport.baseball', 10: 'rec.sport.hockey', 11: 'sci.crypt', 12: 'sci.electronics', 13: 'sci.med', 14: 'sci.space', 15: 'soc.religion.christian', 16: 'talk.politics.guns', 17: 'talk.politics.mideast', 18: 'talk.politics.misc', 19: 'talk.religion.misc'} \n",
      "\n",
      "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'comp.os', 3: 'comp.sys', 4: 'comp.sys', 5: 'comp.windows', 6: 'misc.forsale', 7: 'rec.autos', 8: 'rec.motorcycles', 9: 'rec.sport', 10: 'rec.sport', 11: 'sci.crypt', 12: 'sci.electronics', 13: 'sci.med', 14: 'sci.space', 15: 'soc.religion', 16: 'talk.politics', 17: 'talk.politics', 18: 'talk.politics', 19: 'talk.religion'} \n",
      "\n",
      "{0: 'alt', 1: 'comp', 2: 'comp', 3: 'comp', 4: 'comp', 5: 'comp', 6: 'misc', 7: 'rec', 8: 'rec', 9: 'rec', 10: 'rec', 11: 'sci', 12: 'sci', 13: 'sci', 14: 'sci', 15: 'soc', 16: 'talk', 17: 'talk', 18: 'talk', 19: 'talk'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = generate_mappings(dset)\n",
    "if res:\n",
    "    for mapping in res:\n",
    "        print(mapping, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = ['clusters', 'clusters_2', 'clusters_3']\n",
    "TEXT_COLUMN = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@save_result('df')\n",
    "def prepare_raw_df(mydata, mapping1, mapping2, mapping3):\n",
    "    df = pd.DataFrame({'data': mydata.data, 'target': mydata.target})\n",
    "    for mapping, col in zip([mapping1, mapping2, mapping3], TARGET_COLUMNS):\n",
    "        df[col] = df['target'].apply(lambda x: mapping1[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>clusters</th>\n",
       "      <th>clusters_2</th>\n",
       "      <th>clusters_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>rec.sport</td>\n",
       "      <td>rec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm</td>\n",
       "      <td>comp.sys</td>\n",
       "      <td>comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>talk.politics</td>\n",
       "      <td>talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm</td>\n",
       "      <td>comp.sys</td>\n",
       "      <td>comp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac</td>\n",
       "      <td>comp.sys</td>\n",
       "      <td>comp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  target  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...      10   \n",
       "1  My brother is in the market for a high-perform...       3   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...      17   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3   \n",
       "4  1)    I have an old Jasmine drive which I cann...       4   \n",
       "\n",
       "                clusters     clusters_2 clusters_3  \n",
       "0       rec.sport.hockey      rec.sport        rec  \n",
       "1           comp.sys.ibm       comp.sys       comp  \n",
       "2  talk.politics.mideast  talk.politics       talk  \n",
       "3           comp.sys.ibm       comp.sys       comp  \n",
       "4           comp.sys.mac       comp.sys       comp  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_raw_df(dset, *generate_mappings(dset)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_raw_df(dset, *generate_mappings(dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_duplicate_values(x):\n",
    "    return x.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_frequency(df, column_name):\n",
    "    targets, frequency = np.unique(df[column_name], return_counts=True)\n",
    "    if column_name == TARGET_COLUMNS[2]: # 'clusters_3'\n",
    "        return targets, frequency\n",
    "    return range(len(targets)), frequency\n",
    "\n",
    "def plot_clusters(func, df, title, type='bar'):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(19, 3), sharey=False)\n",
    "    for num, col in enumerate(TARGET_COLUMNS):\n",
    "        if type == 'bar':\n",
    "            axs[num].bar(*func(df, col))\n",
    "        else:\n",
    "            axs[num].scatter(**func(df, col))\n",
    "    fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script False\n",
    "plot_clusters(get_frequency, df, 'Clusters Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script False\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in TARGET_COLUMNS:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])    \n",
    "    save(dict(enumerate(le.classes_)), col)\n",
    "df = df.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def word_lemmatizer(word):\n",
    "    word = word.replace('_', '')\n",
    "    word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\") # NOUNS\n",
    "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\") # VERB\n",
    "    return wordnet_lemmatizer.lemmatize(word2, pos = (\"a\")) # ADJ\n",
    "    \n",
    "\n",
    "def text_lemmatizer(text):\n",
    "    return ' '.join(map(word_lemmatizer, text))\n",
    "\n",
    "\n",
    "def lemmatizer(x):\n",
    "    x[TEXT_COLUMN] = x[TEXT_COLUMN].apply(lambda text: text_lemmatizer(tokenize(remove_stopwords(text))))\n",
    "    return x\n",
    "\n",
    "def get_sentimnent(x):\n",
    "    x['sentimnent'] = x[TEXT_COLUMN].apply(lambda text: vader_analyzer.polarity_scores(text))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_feature_selector(x):\n",
    "    x['word_count'] = x[TEXT_COLUMN].apply(lambda text : len(str(text).split()))\n",
    "    x['length'] = x[TEXT_COLUMN].apply(len)\n",
    "    x['word_density'] = x['length'] / x['word_count']\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script False\n",
    "display(df.shape)\n",
    "df = FunctionTransformer(drop_duplicate_values, validate=False).transform(df)\n",
    "display(df.shape)\n",
    "df = FunctionTransformer(lemmatizer, validate=False).transform(df)\n",
    "display(df.shape)\n",
    "df = FunctionTransformer(text_feature_selector, validate=False).transform(df)\n",
    "display(df.shape)\n",
    "df = FunctionTransformer(get_sentimnent, validate=False).transform(df)\n",
    "display(df.shape)\n",
    "df = pd.concat([df, pd.io.json.json_normalize(df['sentimnent'])], axis=1).drop(columns=['sentimnent'])\n",
    "display(df.shape)\n",
    "\n",
    "df.to_parquet('data/preprocessed_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/preprocessed_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def get_word_density(colname, feature):\n",
    "    tdf = df.groupby([colname]).agg(feature).mean()\n",
    "    return tdf.index, tdf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(partial(get_word_density, feature='word_density') , 'Clusters word_density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_clusters(partial(get_word_density, feature='length'), 'Clusters length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_clusters(partial(get_word_density, feature='word_count'), 'Clusters word_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def clusters_wordcloud(name):\n",
    "    mapping = load(name)\n",
    "    pdf = pd.DataFrame(df.groupby(name).agg('data').sum())\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 25))\n",
    "    for i in range(len(mapping)):\n",
    "        ax = fig.add_subplot(7,3,i+1)\n",
    "        wordcloud = WordCloud().generate(pdf['data'][i])\n",
    "        ax.set_title(\"WordCloud \" + mapping[pdf.index[i]])\n",
    "        ax.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clusters_wordcloud('clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clusters_wordcloud('clusters_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clusters_wordcloud('clusters_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-iDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df[TEXT_COLUMN].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(vectors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduced_results(col, results):\n",
    "    results_pd = pd.concat([df[[col]], pd.DataFrame(data=results, index=df.index, columns=[\"x\", \"y\"])], axis=1)\n",
    "    results_pd = results_pd[results_pd['x'] < 60]\n",
    "    return {\n",
    "        'x': results_pd['x'].values,\n",
    "        'y': results_pd['y'].values,\n",
    "        'c': results_pd[col].values\n",
    "    }\n",
    "\n",
    "def plot_clusters_svd(vectors):\n",
    "    svd = TruncatedSVD(n_components=2, n_iter=7, random_state=42)\n",
    "    plot_clusters(partial(reduced_results, results=svd.fit_transform(vectors)), 'scatter plot for clusters SVD', type='lld')\n",
    "\n",
    "def plot_clusters_two_decompositions(vectors, with_text_features=True, n_components=300):\n",
    "    svd = TruncatedSVD(n_components=15, n_iter=7, random_state=42)\n",
    "    features = svd.fit_transform(vectors)\n",
    "\n",
    "    res = features\n",
    "    if with_text_features:\n",
    "        tmp = df[['word_count', 'length', 'word_density', 'compound', 'neg', 'neu', 'pos']]\n",
    "        tmp = SimpleImputer(missing_values=np.nan, strategy='median').fit_transform(tmp)\n",
    "        tmp = StandardScaler().fit_transform(tmp)\n",
    "        res = np.concatenate([np.array(tmp),features], axis=1)\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    features2 = pca.fit_transform(res)\n",
    "    plot_clusters(partial(reduced_results, results=features2), 'scatter plot for clusters SVD & then PCA', type='lld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clusters_svd(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clusters_two_decompositions(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clusters_two_decompositions(vectors, with_text_features=False, n_components=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clusters_two_decompositions(vectors, with_text_features=False, n_components=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clusters_two_decompositions(vectors, with_text_features=False, n_components=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linkage_matrix(linkage_matrix, pic_save=True, ylimit=None, title=\"ward_clusters\", truncate_mode=None, p=5):\n",
    "    fig, ax = plt.subplots(figsize=(15, 20))\n",
    "    \n",
    "    kwargs = {\n",
    "        'leaf_rotation':90.,  # rotates the x axis labels\n",
    "        'leaf_font_size':8.,  # font size for the x axis labels\n",
    "        'show_contracted':False,  # to get a distribution impression in truncated branches\n",
    "        'show_leaf_counts':False,  # otherwise numbers in brackets are counts\n",
    "        \n",
    "    }\n",
    "    if truncate_mode:\n",
    "        kwargs.update({\n",
    "            'truncate_mode':truncate_mode,  # show only the last p merged clusters\n",
    "            'p':p,  # show only the last p merged clusters\n",
    "        })\n",
    "        \n",
    "    axs = dendrogram(linkage_matrix, **kwargs);\n",
    "\n",
    "    if ylimit:\n",
    "        ax.set_ylim(*ylimit)\n",
    "\n",
    "    plt.tick_params(\n",
    "        axis= 'x',         # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "\n",
    "    plt.tight_layout(); #show plot with tight layout\n",
    "    if pic_save:\n",
    "        plt.savefig('{}.png'.format(title), dpi=500)\n",
    "\n",
    "        \n",
    "def illustrate_dendrogram(vectors, vtype=\"tfidf\"):\n",
    "    #define the linkage_matrix using ward clustering pre-computed distances\n",
    "    linkage_matrix = ward(1 - cosine_similarity(vectors)) \n",
    "    plot_linkage_matrix(linkage_matrix, \n",
    "                        ylimit=(1.2, None), \n",
    "                        truncate_mode='level', p=5, \n",
    "                        title=\"dendrogram_{}_truncate_mode_{}_p_{}_with_ylimit\".format(vtype, 'level', 5))\n",
    "    plot_linkage_matrix(linkage_matrix, title=\"dendrogram_{}_full\".format(vtype))\n",
    "    return linkage_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkage_matrix = illustrate_dendrogram(vectors, vtype=\"tfidf\");\n",
    "# # plot_linkage_matrix(linkage_matrix, ylimit=(1.2, None), truncate_mode='level', p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_linkage_matrix(linkage_matrix, truncate_mode='level', p=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(linkage_matrix, 'tfidf_wrap_cosdist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "editdistance.eval('banana', 'bahama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Y = squareform(pdist(np.array(df['data'].values).reshape(-1,1), np.vectorize(editdistance.eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save(Y, 'levdis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_numbers(s: str) -> str:\n",
    "    \"\"\"Clean digits and punctuation marks from string\n",
    "\n",
    "    :param s: input string\n",
    "    :return: result string\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\W', ' ', ''.join([i for i in s if not i.isdigit()]).strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [clean_numbers(doc)[:1000] for doc in df['data'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_arr = np.array(z).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted({len(b[i][0]) for i in range(len(b))}, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function = np.vectorize(editdistance.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = squareform(pdist(b, function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mapping(q, 'levinstein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwer = '12345678'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwer-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local PySpark (Python-3.5 / Spark-2.3.0 )",
   "language": "python",
   "name": "py3spark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
