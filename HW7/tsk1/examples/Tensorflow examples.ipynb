{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.add(4,2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = 4\n",
    "y = 2\n",
    "\n",
    "op1 = tf.add(x, y)\n",
    "op2 = tf.multiply(x, y)\n",
    "op3 = tf.pow(op2, op1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(op3))\n",
    "\n",
    "try:\n",
    "    from utils import show_current_graph\n",
    "    show_current_graph()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "data = [2, [1, 2], [1, 2, 3]]\n",
    "\n",
    "def get_result(x, y):\n",
    "    op1 = tf.add(x, y)\n",
    "    op2 = tf.add(x, y)\n",
    "    op3 = tf.add(op2, op1)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        return sess.run(op3)\n",
    "    \n",
    "for x, y in combinations(data, 2):\n",
    "    try:\n",
    "        result = get_result(x,y)\n",
    "        print('OK', x, 'and', y, '->', result)\n",
    "    except:\n",
    "        print('Fail', x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([2, 3], name='vector')\n",
    "b = tf.Variable([[0, 1], [2, 3]], name='matrix')\n",
    "W = tf.Variable(tf.zeros(shape=[784, 10]), name='tensor')\n",
    "\n",
    "assign_op = a.assign([1, 2])\n",
    "\n",
    "init_ab = tf.variables_initializer([a, b])\n",
    "init_all_vars = tf.global_variables_initializer() # also you gan get all defined vars using tf.global_variables() func\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(a.initializer) # Initialize a single variable\n",
    "    print(a.eval())\n",
    "    # print(b.eval()) # If you try to execute this op you will get a exception\n",
    "    \n",
    "    sess.run(init_ab) # Initialize only a subset (a,b) of virables\n",
    "    print(b.eval())\n",
    "    \n",
    "    sess.run(init_all_vars) # Initialize all variables at once\n",
    "    print(W.eval())\n",
    "    \n",
    "    sess.run(assign_op)\n",
    "    print(a.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(10)\n",
    "\n",
    "sess1 = tf.Session()\n",
    "sess2 = tf.Session()\n",
    "\n",
    "sess1.run(W.initializer)\n",
    "sess2.run(W.initializer)\n",
    "\n",
    "print(sess1.run(W.assign_add(10)))\n",
    "print(sess2.run(W.assign_add(100)))\n",
    "\n",
    "sess1.close()\n",
    "sess2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "a = tf.Variable(2)\n",
    "b = tf.Variable(2)\n",
    "c = a + b\n",
    "\n",
    "tf.global_variables_initializer().run() # Init all variables\n",
    "\n",
    "print(c.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, shape=[3])\n",
    "b = tf.constant([5, 5, 5], tf.float32)\n",
    "c = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # print(sess.run(c)) # Error because we a doesn't have any value\n",
    "    \n",
    "    feed_dict = {a: [5, 2, 3]}\n",
    "    print(sess.run(c, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple regression mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "N_EPOCHS = 20\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Step 1: init dataset\n",
    "X_data, y_data = make_regression(n_features=1, noise=10, random_state=42)\n",
    "n_samples = X_data.shape[0]\n",
    "\n",
    "# Step 2: create placeholders for X and Y\n",
    "X = tf.placeholder(tf.float32, name='X')\n",
    "Y = tf.placeholder(tf.float32, name='Y')\n",
    "\n",
    "# Step 3: create weight and bias, initialized to 0\n",
    "w = tf.get_variable('weights', initializer=tf.constant(0.0))\n",
    "b = tf.get_variable('bias', initializer=tf.constant(0.0))\n",
    "\n",
    "# Step 4: build model to predict Y\n",
    "Y_predicted = w * X + b \n",
    "\n",
    "# Step 5: use the squared error as the loss function\n",
    "# you can use either mean squared error or Huber loss\n",
    "loss = tf.square(Y - Y_predicted, name='loss')\n",
    "# loss = utils.huber_loss(Y, Y_predicted)\n",
    "\n",
    "# Step 6: using gradient descent with learning rate of 0.001 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Step 7: initialize the necessary variables, in this case, w and b\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    \n",
    "    # Step 8: train the model\n",
    "    for i in range(1, N_EPOCHS+1): \n",
    "        total_loss = 0\n",
    "        for x, y in zip(X_data, y_data):\n",
    "            # Session execute optimizer and fetch values of loss\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: x, Y:y}) \n",
    "            total_loss += l\n",
    "        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            # Step 9: output the values of w and b\n",
    "            w_out, b_out = sess.run([w, b])\n",
    "\n",
    "            plt.close()\n",
    "            plt.plot(X_data, y_data, 'bo', label='Real data')\n",
    "            plt.plot(X_data, X_data * w_out + b_out, 'r', label='Predicted data')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('data/mnist', one_hot=True)\n",
    "X_batch, Y_batch = mnist.train.next_batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_batch[0].reshape((28, 28)), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='image') \n",
    "Y = tf.placeholder(tf.int32, [batch_size, 10], name='label')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.get_variable(name='weights', shape=(784, 10), initializer=tf.random_normal_initializer())\n",
    "b = tf.get_variable(name='bias', shape=(1, 10), initializer=tf.zeros_initializer())\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "# loss = tf.reduce_mean(-tf.reduce_sum(tf.nn.softmax(logits) * tf.log(Y), reduction_indices=[1]))\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Step 7: calculate accuracy with test set\n",
    "preds = tf.nn.softmax(logits)\n",
    "correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    # train the model n_epochs times\n",
    "    for i in range(n_epochs): \n",
    "        total_loss = 0\n",
    "\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            _, loss_batch = sess.run([optimizer, loss], {X: X_batch, Y:Y_batch}) \n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, {X: X_batch, Y:Y_batch})\n",
    "        total_correct_preds += accuracy_batch    \n",
    "\n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from utils import show_current_graph\n",
    "    show_current_graph()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='softmax', input_shape=(784,)))\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(mnist.train.images, mnist.train.labels,\n",
    "          batch_size=batch_size, epochs=5,\n",
    "          validation_data = (mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_test = mnist.test.labels.argmax(axis=1)\n",
    "y_predicted_scores = model.predict(mnist.test.images)\n",
    "y_predicted = y_predicted_scores.argmax(axis=1)\n",
    "\n",
    "print('Classification report\\n')\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_confusion_matrix\n",
    "print_confusion_matrix(confusion_matrix(y_test, y_predicted), range(0, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show incorrect predictions\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "incorrect_classified = y_test != y_predicted\n",
    "\n",
    "incorrect_classified_images = mnist.test.images[incorrect_classified]\n",
    "incorrect_classified_gt = y_test[incorrect_classified]\n",
    "incorrect_classified_scores = y_predicted_scores[incorrect_classified]\n",
    "\n",
    "top_n_examples = 5\n",
    "iter_incorrect = islice(zip(incorrect_classified_images,\n",
    "                            incorrect_classified_gt,\n",
    "                            incorrect_classified_scores),\n",
    "                        top_n_examples)\n",
    "\n",
    "for im, label, score in iter_incorrect:\n",
    "    print('Correct label:', label)\n",
    "    predicted_label = score.argmax()\n",
    "    print('Predicted:', predicted_label, 'with score', score[predicted_label])\n",
    "    plt.imshow(im.reshape((28, 28)), 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DeepDream](http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb)\n",
    "\n",
    "[Neural networks](https://github.com/aymericdamien/TensorFlow-Examples)\n",
    "\n",
    "[TF official examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
